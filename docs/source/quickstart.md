# Quick Start

Get started with FreeInference in 5 minutes.

## Step 1: Get Your API Key

Contact the team to get your FreeInference API key.

## Step 2: Choose Your IDE

### Cursor

1. Open Settings (`Cmd + ,` or `Ctrl + ,`)
2. Go to **API Keys** → Enter your API key
3. Click **Override OpenAI Base URL** → Enter: `https://freeinference.org/v1`
4. Enable the toggle and start coding

[Detailed setup →](integrations.md)

### Codex

1. Create `~/.codex/config.toml`:
   ```toml
   model = "glm-4.6"
   model_provider = "free_inference"
   
   [model_providers.free_inference]
   name = "FreeInference"
   base_url = "https://freeinference.org/v1"
   wire_api = "chat"
   env_http_headers = { "X-Session-ID" = "CODEX_SESSION_ID", "Authorization" = "FREEINFERENCE_API_KEY" }
   ```

2. Add to `~/.zshrc`:
   ```bash
   export CODEX_SESSION_ID="$(date +%Y%m%d-%H%M%S)-$(uuidgen)"
   export FREEINFERENCE_API_KEY="Bearer your-api-key-here"
   ```

3. Reload: `source ~/.zshrc`

[Detailed setup →](integrations.md)

### Roo Code / Kilo Code

1. Install extension in your IDE
2. Settings → **OpenAI Compatible**
3. Base URL: `https://freeinference.org/v1`
4. API Key: `your-api-key-here`

[Detailed setup →](integrations.md)

---

## Step 3: Choose a Model

See [available models](models.md) and select one that fits your needs.

---

## Next Steps

- [Integration Guides](integrations.md) - Detailed setup and troubleshooting
- [Available Models](models.md) - Model specifications and features

## Need Help?

Having issues? Check the [integration guide](integrations.md) for troubleshooting.
